{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block-Gibbs on Dirichlet Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see some of the key ingredients in action in a simple but more realistic setting and write a Dirichlet mixture model in GenJAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Points on the Real Line\n",
    "\n",
    "The goal here is to cluster datapoints on the real line. To do so, we model a fixed number of clusters, each as a 1D-Gaussian with fixed variance, and we want to infer their means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "\n",
    "The \"model of the world\" postulates:\n",
    "- A fixed number of 1D Gaussians\n",
    "- Each Gaussian is assigned a weight, representing the proportion of points assigned to each cluster \n",
    "- Each datapoint is assigned to a cluster\n",
    "\n",
    "### Generative Process\n",
    "\n",
    "We turn this into a generative model as follows:\n",
    "- We have a fixed prior mean and variance for where the cluster centers might be\n",
    "- We sample a mean for each cluster\n",
    "- We sample an initial weight per cluster (sum of weights is 1)\n",
    "- For each datapoint:\n",
    "  - We sample a cluster assignment proportional to the cluster weights\n",
    "  - We sample the datapoint noisily around the mean of the cluster\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "We, the modelers, get to choose how this process is implemented. \n",
    "- We choose distributions for each sampling step in a way that makes **inference tractable**.\n",
    "- More precisely, we choose conjugate pairs so that we can do inference via Gibbs sampling. \n",
    "  - Gibbs sampling is an MCMC method that samples an initial trace, and then updates the traced choices we want to infer over time. \n",
    "  - To update a choice, Gibbs sampling samples from a conditional distribution, which is tractable with conjugate relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genstudio.plot as Plot\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "import genjax\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import categorical, dirichlet, gen, normal, pretty\n",
    "from genjax._src.core.pytree import Const\n",
    "\n",
    "pretty()\n",
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the generative model we described above. It has several hyperparameters that are somewhat manually inferred. An extension to the model could instead do inference over these hyperparameters, and fix hyper-hyperparameters instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "PRIOR_VARIANCE = 10.0\n",
    "OBS_VARIANCE = 1.0\n",
    "N_DATAPOINTS = 5000\n",
    "N_CLUSTERS = 40\n",
    "ALPHA = float(N_DATAPOINTS / (N_CLUSTERS * 10))\n",
    "PRIOR_MEAN = 50.0\n",
    "N_ITER = 50\n",
    "\n",
    "# Debugging mode\n",
    "DEBUG = True\n",
    "\n",
    "\n",
    "# Sub generative functions of the bigger model\n",
    "@gen\n",
    "def generate_cluster(mean, var):\n",
    "    cluster_mean = normal(mean, var) @ \"mean\"\n",
    "    return cluster_mean\n",
    "\n",
    "\n",
    "@gen\n",
    "def generate_cluster_weight(alphas):\n",
    "    probs = dirichlet(alphas) @ \"probs\"\n",
    "    return probs\n",
    "\n",
    "\n",
    "@gen\n",
    "def generate_datapoint(probs, clusters):\n",
    "    idx = categorical(jnp.log(probs)) @ \"idx\"\n",
    "    obs = normal(clusters[idx], OBS_VARIANCE) @ \"obs\"\n",
    "    return obs\n",
    "\n",
    "\n",
    "# Main model\n",
    "@gen\n",
    "def generate_data(n_clusters: Const[int], n_datapoints: Const[int], alpha: float):\n",
    "    clusters = (\n",
    "        generate_cluster.repeat(n=n_clusters.unwrap())(PRIOR_MEAN, PRIOR_VARIANCE)\n",
    "        @ \"clusters\"\n",
    "    )\n",
    "\n",
    "    probs = generate_cluster_weight.inline(\n",
    "        alpha / n_clusters.unwrap() * jnp.ones(n_clusters.unwrap())\n",
    "    )\n",
    "\n",
    "    datapoints = (\n",
    "        generate_datapoint.repeat(n=n_datapoints.unwrap())(probs, clusters)\n",
    "        @ \"datapoints\"\n",
    "    )\n",
    "\n",
    "    return datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some synthetic data to test inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with N_CLUSTERS clusters evenly spaced\n",
    "points_per_cluster = int(N_DATAPOINTS / N_CLUSTERS)\n",
    "cluster_indices = jnp.arange(N_CLUSTERS)\n",
    "offsets = PRIOR_VARIANCE * (-4 + 8 * cluster_indices / N_CLUSTERS)\n",
    "\n",
    "# Create keys for each cluster\n",
    "keys = jax.random.split(jax.random.key(0), N_CLUSTERS)\n",
    "\n",
    "# Generate uniform random points for each cluster\n",
    "uniform_points = jax.vmap(lambda k: jax.random.uniform(k, shape=(points_per_cluster,)))(\n",
    "    keys\n",
    ")\n",
    "\n",
    "# Add offset and prior mean to each cluster's points\n",
    "shifted_points = uniform_points + (PRIOR_MEAN + offsets[:, None])\n",
    "\n",
    "datapoints = C[\"datapoints\", \"obs\"].set(shifted_points.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write the main inference loop. As we said at the beginning, we do MCMC via Gibbs sampling. Inference therefore consist of a main loop and we evolve a trace over time. The final trace contains a sample from the approximate posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(datapoints):\n",
    "    key = jax.random.key(32421)\n",
    "    args = (Const(N_CLUSTERS), Const(N_DATAPOINTS), ALPHA)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    initial_weights = C[\"probs\"].set(jnp.ones(N_CLUSTERS) / N_CLUSTERS)\n",
    "    constraints = datapoints | initial_weights\n",
    "    tr, _ = generate_data.importance(subkey, constraints, args)\n",
    "\n",
    "    if DEBUG:\n",
    "        all_posterior_means = [tr.get_choices()[\"clusters\", \"mean\"]]\n",
    "        all_posterior_weights = [tr.get_choices()[\"probs\"]]\n",
    "        all_cluster_assignment = [tr.get_choices()[\"datapoints\", \"idx\"]]\n",
    "\n",
    "        jax.debug.print(\"Initial means: {v}\", v=all_posterior_means[0])\n",
    "        jax.debug.print(\"Initial weights: {v}\", v=all_posterior_weights[0])\n",
    "\n",
    "        for _ in range(N_ITER):\n",
    "            # Gibbs update on `(\"clusters\", i, \"mean\")` for each i, in parallel\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_cluster_means)(subkey, tr)\n",
    "            all_posterior_means.append(tr.get_choices()[\"clusters\", \"mean\"])\n",
    "\n",
    "            # # Gibbs update on `(\"datapoints\", i, \"idx\")` for each `i`, in parallel\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_datapoint_assignment)(subkey, tr)\n",
    "            all_cluster_assignment.append(tr.get_choices()[\"datapoints\", \"idx\"])\n",
    "\n",
    "            # # Gibbs update on `probs`\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_cluster_weights)(subkey, tr)\n",
    "            all_posterior_weights.append(tr.get_choices()[\"probs\"])\n",
    "\n",
    "        return all_posterior_means, all_posterior_weights, all_cluster_assignment, tr\n",
    "\n",
    "    else:\n",
    "        # One Gibbs sweep consist of updating each latent variable\n",
    "        def update(carry, _):\n",
    "            key, tr = carry\n",
    "            # Gibbs update on `(\"clusters\", i, \"mean\")` for each i, in parallel\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = update_cluster_means(subkey, tr)\n",
    "\n",
    "            # Gibbs update on `(\"datapoints\", i, \"idx\")` for each `i`, in parallel\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = update_datapoint_assignment(subkey, tr)\n",
    "\n",
    "            # Gibbs update on `probs`\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = update_cluster_weights(subkey, tr)\n",
    "            return (key, tr), None\n",
    "\n",
    "        # Overall inference performs a fixed number of Gibbs sweeps\n",
    "        (key, tr), _ = jax.jit(jax.lax.scan)(update, (key, tr), None, length=N_ITER)\n",
    "        return tr\n",
    "\n",
    "\n",
    "def update_cluster_means(key, trace):\n",
    "    # We can update each cluster in parallel\n",
    "    # For each cluster, we find the datapoints in that cluster and compute their mean\n",
    "    datapoint_indexes = trace.get_choices()[\"datapoints\", \"idx\"]\n",
    "    datapoints = trace.get_choices()[\"datapoints\", \"obs\"]\n",
    "    n_clusters = trace.get_args()[0].unwrap()\n",
    "    current_means = trace.get_choices()[\"clusters\", \"mean\"]\n",
    "\n",
    "    # Count number of points per cluster\n",
    "    category_counts = jnp.bincount(\n",
    "        trace.get_choices()[\"datapoints\", \"idx\"],\n",
    "        length=n_clusters,\n",
    "        minlength=n_clusters,\n",
    "    )\n",
    "\n",
    "    # Will contain some NaN due to clusters having no datapoint\n",
    "    cluster_means = (\n",
    "        jax.vmap(\n",
    "            lambda i: jnp.sum(jnp.where(datapoint_indexes == i, datapoints, 0)),\n",
    "            in_axes=(0),\n",
    "            out_axes=(0),\n",
    "        )(jnp.arange(n_clusters))\n",
    "        / category_counts\n",
    "    )\n",
    "\n",
    "    # Conjugate update for Normal-iid-Normal distribution\n",
    "    # See https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture5.pdf\n",
    "    # Note that there's a typo in the math for the posterior mean.\n",
    "    posterior_means = (\n",
    "        PRIOR_VARIANCE\n",
    "        / (PRIOR_VARIANCE + OBS_VARIANCE / category_counts)\n",
    "        * cluster_means\n",
    "        + (OBS_VARIANCE / category_counts)\n",
    "        / (PRIOR_VARIANCE + OBS_VARIANCE / category_counts)\n",
    "        * PRIOR_MEAN\n",
    "    )\n",
    "\n",
    "    posterior_variances = 1 / (1 / PRIOR_VARIANCE + category_counts / OBS_VARIANCE)\n",
    "\n",
    "    # Gibbs resampling of cluster means\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_means = (\n",
    "        generate_cluster.vmap()\n",
    "        .simulate(key, (posterior_means, posterior_variances))\n",
    "        .get_choices()[\"mean\"]\n",
    "    )\n",
    "\n",
    "    # Remove the sampled Nan due to clusters having no datapoint and pick previous mean in that case, i.e. no Gibbs update for them\n",
    "    chosen_means = jnp.where(category_counts == 0, current_means, new_means)\n",
    "\n",
    "    if DEBUG:\n",
    "        jax.debug.print(\"Category counts: {v}\", v=category_counts)\n",
    "        jax.debug.print(\"Current means: {v}\", v=cluster_means)\n",
    "        jax.debug.print(\"Posterior means: {v}\", v=posterior_means)\n",
    "        jax.debug.print(fmt=\"Posterior variance: {v}\", v=posterior_variances)\n",
    "        jax.debug.print(\"Resampled means: {v}\", v=new_means)\n",
    "        jax.debug.print(\"Chosen means: {v}\", v=chosen_means)\n",
    "\n",
    "    argdiffs = genjax.Diff.no_change(trace.args)\n",
    "    new_trace, _, _, _ = trace.update(\n",
    "        subkey, C[\"clusters\", \"mean\"].set(chosen_means), argdiffs\n",
    "    )\n",
    "    return new_trace\n",
    "\n",
    "\n",
    "def update_datapoint_assignment(key, trace):\n",
    "    # We want to update the index for each datapoint, in parallel.\n",
    "    # It means we want to resample the i, but instead of being from the prior\n",
    "    # P(i | probs), we do it from the local posterior P(i | probs, xs).\n",
    "    # We need to do it for all addresses [\"datapoints\", \"idx\", i],\n",
    "    # and as these are independent (when conditioned on the rest)\n",
    "    # we can resample them in parallel.\n",
    "\n",
    "    # Conjugate update for a categorical is just exact posterior via enumeration\n",
    "    # P(x | y ) = P(x, y) \\ sum_x P(x, y).\n",
    "    # P(x | y1, y2) = P(x | y1)\n",
    "    # Sampling from Categorical(P(x = 1 | y ), P(x = 2 | y), ...) is the same as\n",
    "    # sampling from Categorical(P(x = 1, y), P(x = 2, y))\n",
    "    # as the weights need not be normalized\n",
    "    # In addition, if the model factorizes as P(x, y1, y2) = P(x, y1)P(y1 | y2),\n",
    "    # we can further simplify P(y1 | y2) from the categorical as it does not depend on x. More generally We only need to look at the children and parents of x (\"idx\" in our situation, which are conveniently wrapped in the generate_datapoint generative function).\n",
    "    def compute_local_density(x, i):\n",
    "        datapoint_mean = trace.get_choices()[\"datapoints\", \"obs\", x]\n",
    "        chm = C[\"obs\"].set(datapoint_mean).at[\"idx\"].set(i)\n",
    "        clusters = trace.get_choices()[\"clusters\", \"mean\"]\n",
    "        probs = trace.get_choices()[\"probs\"]\n",
    "        args = (probs, clusters)\n",
    "        model_logpdf, _ = generate_datapoint.assess(chm, args)\n",
    "        return model_logpdf\n",
    "\n",
    "    n_clusters = trace.get_args()[0].unwrap()\n",
    "    n_datapoints = trace.get_args()[1].unwrap()\n",
    "    local_densities = jax.vmap(\n",
    "        lambda x: jax.vmap(lambda i: compute_local_density(x, i))(\n",
    "            jnp.arange(n_clusters)\n",
    "        )\n",
    "    )(jnp.arange(n_datapoints))\n",
    "\n",
    "    # Conjugate update by sampling from posterior categorical\n",
    "    # Note: I think we could've used something like\n",
    "    # generate_datapoint.vmap().importance which would perhaps\n",
    "    # work in a more general setting but would definitely be slower here.\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_datapoint_indexes = (\n",
    "        genjax.categorical.vmap().simulate(key, (local_densities,)).get_choices()\n",
    "    )\n",
    "    # Gibbs resampling of datapoint assignment to clusters\n",
    "    argdiffs = genjax.Diff.no_change(trace.args)\n",
    "    new_trace, _, _, _ = trace.update(\n",
    "        subkey, C[\"datapoints\", \"idx\"].set(new_datapoint_indexes), argdiffs\n",
    "    )\n",
    "    return new_trace\n",
    "\n",
    "\n",
    "def update_cluster_weights(key, trace):\n",
    "    # Count number of points per cluster\n",
    "    n_clusters = trace.get_args()[0].unwrap()\n",
    "    category_counts = jnp.bincount(\n",
    "        trace.get_choices()[\"datapoints\", \"idx\"],\n",
    "        length=n_clusters,\n",
    "        minlength=n_clusters,\n",
    "    )\n",
    "\n",
    "    # Conjugate update for Dirichlet distribution\n",
    "    # See https://en.wikipedia.org/wiki/Dirichlet_distribution#Conjugate_to_categorical_or_multinomial\n",
    "    new_alpha = ALPHA / n_clusters * jnp.ones(n_clusters) + category_counts\n",
    "\n",
    "    # Gibbs resampling of cluster weights\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_probs = generate_cluster_weight.simulate(key, (new_alpha,)).get_retval()\n",
    "\n",
    "    if DEBUG:\n",
    "        jax.debug.print(fmt=\"Category counts: {v}\", v=category_counts)\n",
    "        jax.debug.print(fmt=\"New alpha: {v}\", v=new_alpha)\n",
    "        jax.debug.print(fmt=\"New probs: {v}\", v=new_probs)\n",
    "    argdiffs = genjax.Diff.no_change(trace.args)\n",
    "    new_trace, _, _, _ = trace.update(subkey, C[\"probs\"].set(new_probs), argdiffs)\n",
    "    return new_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run inference, obtaining the final trace and some intermediate traces for visualizing inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    (\n",
    "        all_posterior_means,\n",
    "        all_posterior_weights,\n",
    "        all_cluster_assignment,\n",
    "        posterior_trace,\n",
    "    ) = infer(datapoints)\n",
    "else:\n",
    "    posterior_trace = infer(datapoints)\n",
    "\n",
    "posterior_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for the animation\n",
    "data_points = datapoints[\"datapoints\", \"obs\"].tolist()\n",
    "np.random.seed(42)\n",
    "jitter = np.random.uniform(-0.05, 0.05, size=len(data_points)).tolist()\n",
    "std_dev = np.sqrt(OBS_VARIANCE) * 1.5\n",
    "all_cluster_assignments_list = [a.tolist() for a in all_cluster_assignment]\n",
    "all_posterior_means_list = [m.tolist() for m in all_posterior_means]\n",
    "all_posterior_weights_list = [w.tolist() for w in all_posterior_weights]\n",
    "\n",
    "# Define a consistent color palette to use throughout the visualization\n",
    "color_palette = \"\"\"\n",
    "const plotColors = [\n",
    "    \"#4c78a8\", \"#f58518\", \"#e45756\", \"#72b7b2\", \"#54a24b\", \n",
    "    \"#eeca3b\", \"#b279a2\", \"#ff9da6\", \"#9d755d\", \"#bab0ac\",\n",
    "    \"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\",\n",
    "    \"#8c564b\", \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\"\n",
    "];\n",
    "\"\"\"\n",
    "\n",
    "# Shared data initialization for all plot components\n",
    "frame_data_js = (\n",
    "    \"\"\"\n",
    "// Get current frame data\n",
    "const frame = $state.frame;\n",
    "const hoveredCluster = $state.hoveredCluster;\n",
    "const means = \"\"\"\n",
    "    + str(all_posterior_means_list)\n",
    "    + \"\"\"[frame];\n",
    "const weights = \"\"\"\n",
    "    + str(all_posterior_weights_list)\n",
    "    + \"\"\"[frame];\n",
    "const assignments = \"\"\"\n",
    "    + str(all_cluster_assignments_list)\n",
    "    + \"\"\"[frame];\n",
    "const stdDev = \"\"\"\n",
    "    + str(std_dev)\n",
    "    + \"\"\";\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Create a visualizer with animation\n",
    "(\n",
    "    Plot.initialState({\"frame\": 0, \"hoveredCluster\": None})\n",
    "    |\n",
    "    # Main visualization that updates based on the current frame\n",
    "    Plot.plot({\n",
    "        \"marks\": [\n",
    "            # 1. Data points with jitter - show all data points with optional highlighting\n",
    "            Plot.dot(\n",
    "                Plot.js(\n",
    "                    \"\"\"function() {\n",
    "                    \"\"\"\n",
    "                    + frame_data_js\n",
    "                    + \"\"\"\n",
    "                    const dataPoints = \"\"\"\n",
    "                    + str(data_points)\n",
    "                    + \"\"\";\n",
    "                    const jitter = \"\"\"\n",
    "                    + str(jitter)\n",
    "                    + \"\"\";\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    + color_palette\n",
    "                    + \"\"\"\n",
    "                    \n",
    "                    // Return all points with hover-aware opacity\n",
    "                    return dataPoints.map((x, i) => {\n",
    "                        const clusterIdx = assignments[i];\n",
    "                        // If a cluster is hovered, reduce opacity of other clusters' points\n",
    "                        const isHovered = hoveredCluster !== null && clusterIdx === hoveredCluster;\n",
    "                        const opacity = hoveredCluster === null ? 0.5 : (isHovered ? 0.7 : 0.15);\n",
    "                        return {\n",
    "                            x: x,\n",
    "                            y: jitter[i],\n",
    "                            color: plotColors[clusterIdx % 20],\n",
    "                            opacity: opacity\n",
    "                        };\n",
    "                    });\n",
    "                }()\"\"\"\n",
    "                ),\n",
    "                {\"x\": \"x\", \"y\": \"y\", \"fill\": \"color\", \"r\": 3, \"opacity\": \"opacity\"},\n",
    "            ),\n",
    "            # 2. Combined error bars (both horizontal lines and vertical caps)\n",
    "            Plot.line(\n",
    "                Plot.js(\n",
    "                    \"\"\"function() {\n",
    "                    \"\"\"\n",
    "                    + frame_data_js\n",
    "                    + \"\"\"\n",
    "                    const capSize = 0.04;  // Size of the vertical cap lines\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    + color_palette\n",
    "                    + \"\"\"\n",
    "                    \n",
    "                    // We'll collect all line segments in a flat array\n",
    "                    const result = [];\n",
    "                    \n",
    "                    for (let i = 0; i < means.length; i++) {\n",
    "                        // Only include error bars for clusters with weight >= 0.01\n",
    "                        if (weights[i] >= 0.01) {\n",
    "                            // Determine if this cluster is being hovered\n",
    "                            const isHovered = hoveredCluster === i;\n",
    "                            const opacity = hoveredCluster === null ? 0.7 : (isHovered ? 1.0 : 0.3);\n",
    "                            const strokeWidth = isHovered ? 4 : 3;\n",
    "                            const color = plotColors[i % 20];\n",
    "                            \n",
    "                            // Add horizontal line (error bar itself)\n",
    "                            result.push({x: means[i] - stdDev, y: 0, cluster: i, color, opacity, width: strokeWidth});\n",
    "                            result.push({x: means[i] + stdDev, y: 0, cluster: i, color, opacity, width: strokeWidth});\n",
    "                            \n",
    "                            // Add left cap (vertical line)\n",
    "                            result.push({x: means[i] - stdDev, y: -capSize, cluster: i, color, opacity, width: strokeWidth});\n",
    "                            result.push({x: means[i] - stdDev, y: capSize, cluster: i, color, opacity, width: strokeWidth});\n",
    "                            \n",
    "                            // Add right cap (vertical line)\n",
    "                            result.push({x: means[i] + stdDev, y: -capSize, cluster: i, color, opacity, width: strokeWidth});\n",
    "                            result.push({x: means[i] + stdDev, y: capSize, cluster: i, color, opacity, width: strokeWidth});\n",
    "                        }\n",
    "                    }\n",
    "                    return result;\n",
    "                }()\"\"\"\n",
    "                ),\n",
    "                {\n",
    "                    \"x\": \"x\",\n",
    "                    \"y\": \"y\",\n",
    "                    \"stroke\": \"color\",\n",
    "                    \"strokeWidth\": \"width\",\n",
    "                    \"opacity\": \"opacity\",\n",
    "                    \"z\": \"cluster\",\n",
    "                },\n",
    "            ),\n",
    "            # 3. Cluster means as stars\n",
    "            Plot.dot(\n",
    "                Plot.js(\n",
    "                    \"\"\"function() {\n",
    "                    \"\"\"\n",
    "                    + frame_data_js\n",
    "                    + \"\"\"\n",
    "                    \"\"\"\n",
    "                    + color_palette\n",
    "                    + \"\"\"\n",
    "                        \n",
    "                    // Create a simple array for each cluster mean\n",
    "                    return means.map((mean, i) => {\n",
    "                        // Only include means for clusters with sufficient weight\n",
    "                        if (weights[i] >= 0.01) {\n",
    "                            const isHovered = hoveredCluster === i;\n",
    "                            return {\n",
    "                                x: mean,\n",
    "                                y: 0,\n",
    "                                cluster: i,\n",
    "                                color: plotColors[i % 20],\n",
    "                                opacity: isHovered ? 1.0 : 0.8\n",
    "                            };\n",
    "                        }\n",
    "                        return null;  // Skip low-weight clusters\n",
    "                    }).filter(d => d !== null);  // Remove null values\n",
    "                }()\"\"\"\n",
    "                ),\n",
    "                {\n",
    "                    \"x\": \"x\",\n",
    "                    \"y\": \"y\",\n",
    "                    \"fill\": \"color\",\n",
    "                    \"r\": 10,\n",
    "                    \"symbol\": \"star\",\n",
    "                    \"stroke\": \"black\",\n",
    "                    \"strokeWidth\": 2,\n",
    "                    \"opacity\": \"opacity\",\n",
    "                },\n",
    "            ),\n",
    "        ],\n",
    "        \"grid\": True,\n",
    "        \"marginTop\": 40,\n",
    "        \"marginRight\": 40,\n",
    "        \"marginBottom\": 40,\n",
    "        \"marginLeft\": 40,\n",
    "        \"style\": {\"height\": \"400px\"},\n",
    "        \"title\": Plot.js(\n",
    "            \"`Dirichlet Mixture Model - Iteration ${$state.frame} of \"\n",
    "            + str(len(all_posterior_means) - 1)\n",
    "            + \"`\"\n",
    "        ),\n",
    "        \"subtitle\": \"Cluster centers (★) with standard deviation (—) and data points (•)\",\n",
    "    })\n",
    "    |\n",
    "    # Animation controls and legend with hover effects\n",
    "    Plot.html([\n",
    "        \"div\",\n",
    "        {\"className\": \"p-4\"},\n",
    "        [\n",
    "            \"div\",\n",
    "            {\"className\": \"mb-4\"},\n",
    "            Plot.Slider(\n",
    "                \"frame\",\n",
    "                init=0,\n",
    "                range=[0, len(all_posterior_means) - 1],\n",
    "                step=1,\n",
    "                label=\"Iteration\",\n",
    "                width=\"100%\",\n",
    "                fps=8,\n",
    "            ),\n",
    "        ],\n",
    "        [\n",
    "            \"div\",\n",
    "            {\"className\": \"mt-4\"},\n",
    "            Plot.js(\n",
    "                \"\"\"function() {\n",
    "                \"\"\"\n",
    "                + frame_data_js\n",
    "                + \"\"\"\n",
    "                // Count assignments in current frame\n",
    "                const counts = {};\n",
    "                assignments.forEach(a => { counts[a] = (counts[a] || 0) + 1; });\n",
    "                \n",
    "                \"\"\"\n",
    "                + color_palette\n",
    "                + \"\"\"\n",
    "                \n",
    "                // Sort clusters by weight, filter by minimum weight, and limit to top 10\n",
    "                const topClusters = Object.keys(weights)\n",
    "                    .map(i => ({ \n",
    "                        id: parseInt(i), \n",
    "                        weight: weights[i], \n",
    "                        count: counts[parseInt(i)] || 0 \n",
    "                    }))\n",
    "                    .filter(c => c.weight >= 0.01)\n",
    "                    .sort((a, b) => b.weight - a.weight)\n",
    "                    .slice(0, 10);\n",
    "                \n",
    "                // Create placeholder rows for consistent height\n",
    "                const placeholders = Array(Math.max(0, 10 - topClusters.length))\n",
    "                    .fill(0)\n",
    "                    .map(() => [\"tr\", {\"className\": \"h-8\"}, [\"td\", {\"colSpan\": 3}, \"\"]]);\n",
    "                    \n",
    "                return [\n",
    "                    \"div\", {},\n",
    "                    [\"h3\", {}, `Top Clusters by Weight (Iteration ${frame})`],\n",
    "                    [\"div\", {\"style\": {\"height\": \"280px\", \"overflow\": \"auto\"}},\n",
    "                        [\"table\", {\"className\": \"w-full mt-2\"},\n",
    "                            [\"thead\", [\"tr\",\n",
    "                                [\"th\", {\"className\": \"text-left\"}, \"Cluster\"],\n",
    "                                [\"th\", {\"className\": \"text-left\"}, \"Weight\"],\n",
    "                                [\"th\", {\"className\": \"text-left\"}, \"Points\"]\n",
    "                            ]],\n",
    "                            [\"tbody\",\n",
    "                                ...topClusters.map(cluster => \n",
    "                                    [\"tr\", {\n",
    "                                        \"className\": \"h-8\",\n",
    "                                        \"style\": {\n",
    "                                            \"cursor\": \"pointer\",\n",
    "                                            \"backgroundColor\": $state.hoveredCluster === cluster.id ? \"#f0f0f0\" : \"transparent\"\n",
    "                                        },\n",
    "                                        \"onMouseEnter\": () => { $state.hoveredCluster = cluster.id; },\n",
    "                                        \"onMouseLeave\": () => { $state.hoveredCluster = null; }\n",
    "                                    },\n",
    "                                    [\"td\", {\"className\": \"py-1\"}, \n",
    "                                        [\"div\", {\"className\": \"flex items-center\"},\n",
    "                                            [\"div\", {\n",
    "                                                \"style\": {\n",
    "                                                    \"backgroundColor\": plotColors[cluster.id % 20],\n",
    "                                                    \"width\": \"24px\",\n",
    "                                                    \"height\": \"24px\",\n",
    "                                                    \"borderRadius\": \"4px\",\n",
    "                                                    \"border\": \"1px solid rgba(0,0,0,0.2)\",\n",
    "                                                    \"display\": \"inline-block\",\n",
    "                                                    \"marginRight\": \"8px\"\n",
    "                                                }\n",
    "                                            }],\n",
    "                                            `Cluster ${cluster.id}`\n",
    "                                        ]\n",
    "                                    ],\n",
    "                                    [\"td\", {\"className\": \"py-1\"}, cluster.weight.toFixed(4)],\n",
    "                                    [\"td\", {\"className\": \"py-1\"}, cluster.count]\n",
    "                                    ]\n",
    "                                ),\n",
    "                                ...placeholders\n",
    "                            ]\n",
    "                        ]\n",
    "                    ]\n",
    "                ];\n",
    "            }()\"\"\"\n",
    "            ),\n",
    "        ],\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the interested reader, here's some exercises to try out to make this model better:\n",
    "1) Extend the model to infer the variance of the clusters by putting an inverse_gamma prior replacing the `OBS_VARIANCE` hyperparameter and doing block-Gibbs on it using the normal-inverse-gamma conjugacy\n",
    "2) Try a better initialization of the datapoint assignment: pick a point a use something like k-means and assign all the surrounding points to the same initial cluster. Iterate on all the points until they all have some initial cluster.\n",
    "3) Improve inference using SMC via data annealing: subssample 1/100 of the data and run inference on this, then run inference again on 1/10 of the data starting with the inferred choices for cluster means and weights from the previous trace, and finally repeat for the whole data.\n",
    "\n",
    "Note that the model is still expected to get stuck in local minima (the clustering at the borders isn't great), and one way to improve upon it would be to use a split-merge move, via reversible-jump MCMC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
